{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b17c259-af8b-418d-9426-3d23420cb8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from my_snn.tonic_dataloader import DatasetLoader\n",
    "from my_snn.abstract_rsnn import CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291db3c7-2bcd-4032-9032-e6bb6880a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'shd'\n",
    "time_window = 250\n",
    "batch_size = 64 # lr=1e-4\n",
    "#batch_size = 128 # lr=1e-4\n",
    "DL = DatasetLoader(dataset=dataset, caching='memory', num_workers=0, batch_size=batch_size, time_window=time_window)\n",
    "test_loader, train_loader = DL.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff38f38d-249f-4308-81e2-e1b4a4f6b71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_type:  MG\n",
      "hight:  0.15 ;scale:  6.0\n",
      "device: cuda:0\n",
      "epoch:  1 . Loss:  1.1407620906829834 . Tr Accuracy:  31.2623031496063 . Ts Accuracy:  65.625 Fr:  0.13224804\n",
      "epoch:  2 . Loss:  0.6206505298614502 . Tr Accuracy:  71.55511811023622 . Ts Accuracy:  74.24107142857143 Fr:  0.13871191\n",
      "epoch:  3 . Loss:  0.3103634715080261 . Tr Accuracy:  82.43110236220473 . Ts Accuracy:  75.26785714285714 Fr:  0.13193847\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model/model_81.74107142857143-readout-2layer-v2-4ms.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-279dd34b1e2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[1;31m###############################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' Accuracy: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-279dd34b1e2a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, num_epochs)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mts_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mts_acc\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_accuracy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./model/model_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mts_acc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'-readout-2layer-v2-4ms.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m             \u001b[0mbest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mts_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearnig\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearnig\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearnig\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/model_81.74107142857143-readout-2layer-v2-4ms.pth'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "torch.manual_seed(2)\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "decay = 0.1  # neuron decay rate\n",
    "thresh = 0.5  # neuronal threshold\n",
    "lens = 0.5  # hyper-parameters of approximate function\n",
    "num_epochs = 20  # 150  # n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "'''\n",
    "STEP 3a: CREATE spike MODEL CLASS\n",
    "'''\n",
    "\n",
    "b_j0 = 0.01  # neural threshold baseline\n",
    "R_m = 1  # membrane resistance\n",
    "dt = 1  #\n",
    "gamma = .5  # gradient scale\n",
    "\n",
    "# define approximate firing function\n",
    "\n",
    "gradient_type = 'MG'\n",
    "print('gradient_type: ',gradient_type)\n",
    "scale = 6.\n",
    "hight = 0.15\n",
    "print('hight: ',hight,';scale: ',scale)\n",
    "\n",
    "def gaussian(x, mu=0., sigma=.5):\n",
    "    return torch.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / torch.sqrt(2 * torch.tensor(math.pi)) / sigma\n",
    "\n",
    "\n",
    "# define approximate firing function\n",
    "\n",
    "class ActFun_adp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):  # input = membrane potential- threshold\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.gt(0).float()  # is firing ???\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):  # approximate the gradients\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        # temp = abs(input) < lens\n",
    "  \n",
    "        if gradient_type == 'G':\n",
    "            temp = torch.exp(-(input**2)/(2*lens**2))/torch.sqrt(2*torch.tensor(math.pi))/lens\n",
    "        elif gradient_type == 'MG':\n",
    "            temp = gaussian(input, mu=0., sigma=lens) * (1. + hight) \\\n",
    "                - gaussian(input, mu=lens, sigma=scale * lens) * hight \\\n",
    "                - gaussian(input, mu=-lens, sigma=scale * lens) * hight\n",
    "        elif gradient_type =='linear':\n",
    "            temp = F.relu(1-input.abs())\n",
    "        elif gradient_type == 'slayer':\n",
    "            temp = torch.exp(-5*input.abs())\n",
    "        return grad_input * temp.float() * gamma\n",
    "\n",
    "act_fun_adp = ActFun_adp.apply\n",
    "# tau_m = torch.FloatTensor([tau_m])\n",
    "\n",
    "def mem_update_adp(inputs, mem, spike, tau_adp, b, tau_m, dt=1, isAdapt=1):\n",
    "    alpha = torch.exp(-1. * dt / tau_m).cuda()\n",
    "    ro = torch.exp(-1. * dt / tau_adp).cuda()\n",
    "    if isAdapt:\n",
    "        beta = 1.8\n",
    "    else:\n",
    "        beta = 0.\n",
    "\n",
    "    b = ro * b + (1 - ro) * spike\n",
    "    B = b_j0 + beta * b\n",
    "\n",
    "    mem = mem * alpha + (1 - alpha) * R_m * inputs - B * spike * dt\n",
    "    inputs_ = mem - B\n",
    "    spike = act_fun_adp(inputs_)  # act_fun : approximation firing function\n",
    "    return mem, spike, B, b\n",
    "\n",
    "\n",
    "def output_Neuron(inputs, mem, tau_m, dt=1):\n",
    "    \"\"\"\n",
    "    The read out neuron is leaky integrator without spike\n",
    "    \"\"\"\n",
    "    # alpha = torch.exp(-1. * dt / torch.FloatTensor([30.])).cuda()\n",
    "    alpha = torch.exp(-1. * dt / tau_m).cuda()\n",
    "    mem = mem * alpha + (1. - alpha) * R_m * inputs\n",
    "    return mem\n",
    "\n",
    "\n",
    "class RNN_custom(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_custom, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        # self.hidden_size = input_size\n",
    "        self.i_2_h1 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.h1_2_h1 = nn.Linear(hidden_size[0], hidden_size[0])\n",
    "        self.h1_2_h2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.h2_2_h2 = nn.Linear(hidden_size[1], hidden_size[1])\n",
    "\n",
    "        self.h2o = nn.Linear(hidden_size[1], output_size)\n",
    "\n",
    "        self.tau_adp_h1 = nn.Parameter(torch.Tensor(hidden_size[0]))\n",
    "        self.tau_adp_h2 = nn.Parameter(torch.Tensor(hidden_size[1]))\n",
    "        self.tau_adp_o = nn.Parameter(torch.Tensor(output_size))\n",
    "        self.tau_m_h1 = nn.Parameter(torch.Tensor(hidden_size[0]))\n",
    "        self.tau_m_h2 = nn.Parameter(torch.Tensor(hidden_size[1]))\n",
    "        self.tau_m_o = nn.Parameter(torch.Tensor(output_size))\n",
    "\n",
    "        # nn.init.orthogonal_(self.h1_2_h1.weight)\n",
    "        # nn.init.orthogonal_(self.h2_2_h2.weight)\n",
    "        nn.init.orthogonal_(self.h1_2_h1.weight)\n",
    "        nn.init.orthogonal_(self.h2_2_h2.weight)\n",
    "        nn.init.xavier_uniform_(self.i_2_h1.weight)\n",
    "        nn.init.xavier_uniform_(self.h1_2_h2.weight)\n",
    "        nn.init.xavier_uniform_(self.h2o.weight)\n",
    "\n",
    "        nn.init.constant_(self.i_2_h1.bias, 0)\n",
    "        nn.init.constant_(self.h1_2_h2.bias, 0)\n",
    "        nn.init.constant_(self.h2_2_h2.bias, 0)\n",
    "        nn.init.constant_(self.h1_2_h1.bias, 0)\n",
    "\n",
    "        nn.init.normal_(self.tau_adp_h1,150,10)\n",
    "        nn.init.normal_(self.tau_adp_h2, 150,10)\n",
    "        nn.init.normal_(self.tau_adp_o, 150,10)\n",
    "        nn.init.normal_(self.tau_m_h1, 20.,5)\n",
    "        nn.init.normal_(self.tau_m_h2, 20.,5)\n",
    "        nn.init.normal_(self.tau_m_o, 20.,5)\n",
    "\n",
    "        self.dp = nn.Dropout(0.1)\n",
    "\n",
    "        self.b_h1 = self.b_h2 = self.b_o = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, seq_num, input_dim = input.shape\n",
    "        self.b_h1 = self.b_h2 = self.b_o = b_j0\n",
    "        # mem_layer1 = spike_layer1 = torch.zeros(batch_size, self.hidden_size[0]).cuda()\n",
    "        # mem_layer2 = spike_layer2 = torch.zeros(batch_size, self.hidden_size[1]).cuda()\n",
    "        mem_layer1 = torch.rand(batch_size, self.hidden_size[0]).cuda()\n",
    "        mem_layer2 = torch.rand(batch_size, self.hidden_size[1]).cuda()\n",
    "\n",
    "        spike_layer1 = torch.zeros(batch_size, self.hidden_size[0]).cuda()\n",
    "        spike_layer2 = torch.zeros(batch_size, self.hidden_size[1]).cuda()\n",
    "        mem_output = torch.rand(batch_size, output_dim).cuda()\n",
    "        output = torch.zeros(batch_size, output_dim).cuda()\n",
    "\n",
    "        hidden_spike_ = []\n",
    "        hidden_mem_ = []\n",
    "        h2o_mem_ = []\n",
    "\n",
    "        for i in range(seq_num):\n",
    "            input_x = input[:, i, :]\n",
    "\n",
    "            h_input = self.i_2_h1(input_x.float()) + self.h1_2_h1(spike_layer1)\n",
    "            mem_layer1, spike_layer1, theta_h1, self.b_h1 = mem_update_adp(h_input, mem_layer1, spike_layer1,\n",
    "                                                                         self.tau_adp_h1, self.b_h1,self.tau_m_h1)\n",
    "            # spike_layer1 = self.dp(spike_layer1)\n",
    "            h2_input = self.h1_2_h2(spike_layer1) + self.h2_2_h2(spike_layer2)\n",
    "            mem_layer2, spike_layer2, theta_h2, self.b_h2 = mem_update_adp(h2_input, mem_layer2, spike_layer2,\n",
    "                                                                         self.tau_adp_h2, self.b_h2, self.tau_m_h2)\n",
    "            mem_output = output_Neuron(self.h2o(spike_layer2), mem_output, self.tau_m_o)\n",
    "            if i > 10:\n",
    "                output= output + F.softmax(mem_output, dim=1)#F.softmax(mem_output, dim=1)#\n",
    "\n",
    "            hidden_spike_.append(spike_layer1.data.cpu().numpy())\n",
    "            hidden_mem_.append(mem_layer1.data.cpu().numpy())\n",
    "            h2o_mem_.append(mem_output.data.cpu().numpy())\n",
    "\n",
    "        return output, hidden_spike_, hidden_mem_, h2o_mem_\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 700\n",
    "hidden_dim = [128,128]  # 128\n",
    "output_dim = 20\n",
    "seq_dim = 250  # Number of steps to unroll\n",
    "num_encode = 700\n",
    "total_steps = seq_dim\n",
    "\n",
    "model = RNN_custom(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate =  1e-2\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,eps=1e-5)\n",
    "\n",
    "# base_params = [model.i_2_h1.weight, model.i_2_h1.bias,\n",
    "#                model.h1_2_h1.weight, model.h1_2_h1.bias,\n",
    "#                model.h1_2_h2.weight, model.h1_2_h2.bias,\n",
    "#                model.h2_2_h2.weight, model.h2_2_h2.bias,\n",
    "#                model.h2o.weight, model.h2o.bias]\n",
    "# optimizer = torch.optim.Adam([\n",
    "#     {'params': base_params},\n",
    "#     {'params': model.tau_adp_h1, 'lr': learning_rate * 5},\n",
    "#     {'params': model.tau_adp_h2, 'lr': learning_rate * 5},\n",
    "#     {'params': model.tau_m_h1, 'lr': learning_rate * 1},\n",
    "#     {'params': model.tau_m_h2, 'lr': learning_rate * 1},\n",
    "#     {'params': model.tau_m_o, 'lr': learning_rate * 1}],\n",
    "#     lr=learning_rate,eps=1e-5)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=.5)\n",
    "\n",
    "path = os.path.join(CHECKPOINT_PATH,'bojian_model')  # .pth'\n",
    "\n",
    "def train(model, num_epochs=150):\n",
    "    acc = []\n",
    "    best_accuracy = 80\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(1,num_epochs):\n",
    "        loss_sum = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            #images = images.view(-1, seq_dim, input_dim).requires_grad_().to(device)\n",
    "            images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "            labels = labels.long().to(device)\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass to get output/logits\n",
    "            outputs, _,_,_ = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # Calculate Loss: softmax --> cross entropy loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_sum+= loss\n",
    "            # Getting gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            total += labels.size(0)\n",
    "            if torch.cuda.is_available():\n",
    "                correct += (predicted.cpu() == labels.long().cpu()).sum()\n",
    "            else:\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "        scheduler.step()\n",
    "        accuracy = 100. * correct.numpy() / total\n",
    "        # accuracy,_ = test(model, train_loader)\n",
    "        ts_acc,fr = test(model,is_test=0)\n",
    "        if ts_acc > best_accuracy and accuracy > 80:\n",
    "            torch.save(model, './model/model_' + str(ts_acc) + '-readout-2layer-v2-4ms.pth')\n",
    "            best_accuracy = ts_acc\n",
    " \n",
    "        print('epoch: ', epoch, '. Loss: ', loss.item(), '. Tr Accuracy: ', accuracy, '. Ts Accuracy: ',\n",
    "         ts_acc, 'Fr: ',fr)\n",
    "\n",
    "        acc.append(accuracy)\n",
    "        # if epoch %5==0:\n",
    "        #     print('epoch: ', epoch, '. Loss: ', loss_sum.item()/i, \n",
    "        #             '. Tr Accuracy: ', accuracy, '. Ts Accuracy: ', ts_acc,', Fr: ',fr)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def test(model, dataloader=test_loader,is_test=0):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Iterate through test dataset\n",
    "    for images, labels in dataloader:\n",
    "        images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "        _, labels = torch.max(labels.data, 1)\n",
    "\n",
    "        outputs, fr_,_,_ = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        if torch.cuda.is_available():\n",
    "            correct += (predicted.cpu() == labels.long().cpu()).sum()\n",
    "        else:\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "    accuracy = 100. * correct.numpy() / total\n",
    "    if is_test:\n",
    "        print('Mean FR: ', np.array(fr_).mean())\n",
    "    return accuracy, np.array(fr_).mean()\n",
    "\n",
    "\n",
    "###############################\n",
    "acc = train(model, num_epochs)\n",
    "test_acc,fr = test(model,is_test=1)\n",
    "print(' Accuracy: ', test_acc)\n",
    "\n",
    "\n",
    "\n",
    "# dataset shape:  (8156, 250, 700)\n",
    "# dataset shape:  (2264, 250, 700)\n",
    "# gradient_type:  MG\n",
    "# hight:  0.15 ;scale:  6.0\n",
    "# device: cuda:0\n",
    "# epoch:  1 . Loss:  1.4047224521636963 . Tr Accuracy:  30.946542422756252 . Ts Accuracy:  40.90106007067138 Fr:  0.10361849\n",
    "# epoch:  2 . Loss:  0.5009759664535522 . Tr Accuracy:  58.26385483079941 . Ts Accuracy:  75.79505300353357 Fr:  0.14015365\n",
    "# epoch:  3 . Loss:  0.8235954642295837 . Tr Accuracy:  80.44384502206964 . Ts Accuracy:  86.17491166077738 Fr:  0.12855339\n",
    "# epoch:  4 . Loss:  0.1801595240831375 . Tr Accuracy:  85.93673369298676 . Ts Accuracy:  78.00353356890459 Fr:  0.12772396\n",
    "# epoch:  5 . Loss:  0.6335716843605042 . Tr Accuracy:  87.84943599803826 . Ts Accuracy:  83.25971731448763 Fr:  0.122699216\n",
    "# epoch:  6 . Loss:  0.4184652864933014 . Tr Accuracy:  88.46248160863168 . Ts Accuracy:  85.02650176678445 Fr:  0.12641016\n",
    "# epoch:  7 . Loss:  0.11178665608167648 . Tr Accuracy:  91.66257969592938 . Ts Accuracy:  83.70141342756183 Fr:  0.11583985\n",
    "# epoch:  8 . Loss:  0.24507765471935272 . Tr Accuracy:  93.80823933300637 . Ts Accuracy:  80.43286219081273 Fr:  0.11689453\n",
    "# epoch:  9 . Loss:  0.02580219879746437 . Tr Accuracy:  93.6856302108877 . Ts Accuracy:  78.53356890459364 Fr:  0.11530859\n",
    "# epoch:  10 . Loss:  0.132295161485672 . Tr Accuracy:  95.15693967631192 . Ts Accuracy:  87.0583038869258 Fr:  0.107161455\n",
    "# epoch:  11 . Loss:  0.052720583975315094 . Tr Accuracy:  97.4619911721432 . Ts Accuracy:  87.54416961130742 Fr:  0.111710936\n",
    "# epoch:  12 . Loss:  0.013912809081375599 . Tr Accuracy:  97.63364394310936 . Ts Accuracy:  88.29505300353357 Fr:  0.11046224\n",
    "# epoch:  13 . Loss:  0.013542967848479748 . Tr Accuracy:  98.43060323688083 . Ts Accuracy:  84.31978798586573 Fr:  0.11176432\n",
    "# epoch:  14 . Loss:  0.03696903958916664 . Tr Accuracy:  98.14860225600785 . Ts Accuracy:  85.95406360424029 Fr:  0.113291666\n",
    "# epoch:  15 . Loss:  0.12882192432880402 . Tr Accuracy:  98.4428641490927 . Ts Accuracy:  84.67314487632508 Fr:  0.11321094\n",
    "# epoch:  16 . Loss:  0.008622714318335056 . Tr Accuracy:  98.60225600784699 . Ts Accuracy:  87.72084805653711 Fr:  0.11448698\n",
    "# epoch:  17 . Loss:  0.08007299154996872 . Tr Accuracy:  98.58999509563512 . Ts Accuracy:  85.07067137809187 Fr:  0.109397136\n",
    "# epoch:  18 . Loss:  0.17344336211681366 . Tr Accuracy:  97.91564492398234 . Ts Accuracy:  86.43992932862191 Fr:  0.114264324\n",
    "# epoch:  19 . Loss:  0.31412410736083984 . Tr Accuracy:  97.89112309955861 . Ts Accuracy:  90.68021201413427 Fr:  0.110308595\n",
    "# Mean FR:  0.10995052\n",
    "#  Accuracy:  90.7243816254417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485d6db-fbc5-4f62-a728-61608b1765c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
